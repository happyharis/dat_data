{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scrapy\n",
    "import csv\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
    "from googlesearch import search\n",
    "logging.getLogger('scrapy').propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(tag, n, language):\n",
    "    urls = [url for url in search(tag, stop=n, lang=language)][:n]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MailSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'email'\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        links = LxmlLinkExtractor(allow=()).extract_links(response)\n",
    "        links = [str(link.url) for link in links]\n",
    "        links.append(str(response.url))\n",
    "        \n",
    "        for link in links:\n",
    "            yield scrapy.Request(url=link, callback=self.parse_link) \n",
    "            \n",
    "    def parse_link(self, response):\n",
    "        \n",
    "        for word in self.reject:\n",
    "            if word in str(response.url):\n",
    "                return\n",
    "            \n",
    "        html_text = str(response.text)\n",
    "#         mail_list = re.findall(\"^\\w+([-+.']\\w+)*@[A-Za-z\\d]+\\.(?:com|sg|edu)\", html_text)\n",
    "        mail_list = re.findall('\\w+@\\w+\\.{1}\\w+', html_text)\n",
    "\n",
    "        dic = {'email': mail_list, 'link': str(response.url)}\n",
    "        df = pd.DataFrame(dic)\n",
    "        \n",
    "        df.to_csv(self.path, mode='a', header=False)\n",
    "        df.to_csv(self.path, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user(question):\n",
    "    response = input(question + ' y/n' + '\\n')\n",
    "    if response == 'y':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def create_file(path):\n",
    "    response = False\n",
    "    if os.path.exists(path):\n",
    "        response = ask_user('File already exists, replace?')\n",
    "        if response == False: return \n",
    "    \n",
    "    with open(path, 'wb') as file: \n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(tag, n, language, path, reject=[]):\n",
    "    \n",
    "    create_file(path)\n",
    "    df = pd.DataFrame(columns=['email', 'link'], index=[0])\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    print('Collecting Google urls...')\n",
    "    google_urls = get_urls(tag, n, language)\n",
    "#     google_urls = ['http://ecube.com.sg/']\n",
    "    \n",
    "    print('Searching for emails...')\n",
    "    process = CrawlerProcess({'USER_AGENT': 'Mozilla/5.0'})\n",
    "    process.crawl(MailSpider, start_urls=google_urls, path=path, reject=reject)\n",
    "    process.start()\n",
    "    \n",
    "    print('Cleaning emails...')\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.columns = ['email', 'link']\n",
    "    df = df.drop_duplicates(subset='email')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emails(websites, path, reject=[]):\n",
    "    \n",
    "    create_file(path)\n",
    "    df = pd.DataFrame(columns=['email', 'link'], index=[0])\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    print('Searching for emails...')\n",
    "    process = CrawlerProcess({'USER_AGENT': 'Mozilla/5.0'})\n",
    "    process.crawl(MailSpider, start_urls=websites, path=path, reject=reject)\n",
    "    process.start()\n",
    "    \n",
    "    print('Cleaning emails...')\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.columns = ['email', 'link']\n",
    "    df = df.drop_duplicates(subset='email')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_list = [];\n",
    "\n",
    "with open('tuition_websites.csv', newline='') as csvfile:\n",
    "    websites = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in websites:\n",
    "        websites_list.append(row[0])\n",
    "        \n",
    "formatted_websites = websites_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_words = ['facebook', 'instagram', 'youtube', 'twitter', 'wiki']\n",
    "# df = get_info('Walter Education Centre', 10, 'en', 'tuition_emails.csv', reject=bad_words)\n",
    "# emails = get_emails(formatted_websites, 'tuition_emails.csv', reject=bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_centre_names = []\n",
    "\n",
    "with open('tuition_names.csv') as csvfile:\n",
    "    tuitions = csv.reader(csvfile)\n",
    "    for row in tuitions:\n",
    "        name = ' '.join(row)\n",
    "        education_centre_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuition_urls(list_of_keywords, path):\n",
    "\n",
    "    create_file(path)\n",
    "    tuitions = []\n",
    "    length_of_keywords = len(list_of_keywords)\n",
    "    for keyword in list_of_keywords[:10]:\n",
    "        google_url = get_urls(keyword, 1, 'en');\n",
    "        tuitions.append((keyword, google_url[0]))\n",
    "        progress = 'In progress: ' + str(list_of_keywords.index(keyword) + 1) +'/'+  str(length_of_keywords);\n",
    "        print(progress,  end='\\r')\n",
    "        \n",
    "    tuition_weblinks = pd.DataFrame(tuitions, columns = ['name' , 'url'])\n",
    "    tuition_weblinks.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists, replace? y/n\n",
      "y\n",
      "In progress: 10/1469\r"
     ]
    }
   ],
   "source": [
    "get_tuition_urls(education_centre_names, 'tuition_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
